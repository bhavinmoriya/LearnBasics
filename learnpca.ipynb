{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhavinmoriya/learnpca?scriptVersionId=257396417\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"11aeca7c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.002061,"end_time":"2025-08-21T21:19:55.241713","exception":false,"start_time":"2025-08-21T21:19:55.239652","status":"completed"},"tags":[]},"source":["# **Principal Component Analysis (PCA) in Python**\n","\n","**Principal Component Analysis (PCA)** is a **dimensionality reduction** technique that transforms high-dimensional data into a lower-dimensional form while preserving as much variability as possible. It is widely used for **exploratory data analysis, noise reduction, and feature extraction**.\n","\n","---\n","\n","## **1. What is PCA?**\n","- PCA identifies **patterns** in data by finding directions (principal components) where the data varies the most.\n","- It **projects** data onto these directions, reducing the number of features while retaining most of the information.\n","- The first principal component captures the **maximum variance**, the second captures the next highest variance (orthogonal to the first), and so on.\n","\n","---\n","\n","## **2. Key Concepts**\n","- **Principal Components (PCs)**: New features created by PCA, ordered by the amount of variance they explain.\n","- **Eigenvalues**: Represent the magnitude of variance captured by each principal component.\n","- **Eigenvectors**: Define the direction of the principal components.\n","- **Explained Variance**: The proportion of total variance explained by each principal component.\n","\n","---\n","\n","## **3. Steps to Perform PCA in Python**\n","### **Step 1: Import Required Libraries**\n","```python\n","import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","```\n","\n","### **Step 2: Load and Prepare Data**\n","PCA works best with **standardized data** (mean=0, variance=1).\n","\n","```python\n","# Example dataset (replace with your data)\n","data = pd.DataFrame({\n","    'Feature1': [1, 2, 3, 4, 5],\n","    'Feature2': [5, 4, 3, 2, 1],\n","    'Feature3': [2, 3, 4, 5, 6]\n","})\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data)\n","```\n","\n","---\n","\n","### **Step 3: Apply PCA**\n","```python\n","# Initialize PCA (choose number of components)\n","pca = PCA(n_components=2)  # Reduce to 2 dimensions\n","\n","# Fit and transform the data\n","principal_components = pca.fit_transform(scaled_data)\n","```\n","\n","---\n","\n","### **Step 4: Explained Variance**\n","```python\n","# Explained variance ratio\n","print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n","\n","# Cumulative explained variance\n","cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n","print(\"Cumulative explained variance:\", cumulative_variance)\n","```\n","\n","#### **Output Example:**\n","```\n","Explained variance ratio: [0.924, 0.075]  # 92.4% variance explained by PC1, 7.5% by PC2\n","Cumulative explained variance: [0.924, 0.999]  # 99.9% total variance explained by 2 PCs\n","```\n","\n","---\n","\n","### **Step 5: Visualize Principal Components**\n","```python\n","# Create a DataFrame for the principal components\n","pc_df = pd.DataFrame(\n","    data=principal_components,\n","    columns=['PC1', 'PC2']\n",")\n","\n","# Plot the principal components\n","plt.figure(figsize=(8, 6))\n","plt.scatter(pc_df['PC1'], pc_df['PC2'])\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA: Principal Components')\n","plt.grid()\n","plt.show()\n","```\n","\n","---\n","\n","## **4. Choosing the Number of Components**\n","### **Scree Plot**\n","A scree plot helps visualize the explained variance to decide how many components to keep.\n","\n","```python\n","# Plot explained variance\n","plt.figure(figsize=(8, 6))\n","plt.bar(\n","    range(1, len(pca.explained_variance_ratio_) + 1),\n","    pca.explained_variance_ratio_,\n","    alpha=0.5,\n","    align='center',\n","    label='Individual explained variance'\n",")\n","plt.step(\n","    range(1, len(cumulative_variance) + 1),\n","    cumulative_variance,\n","    where='mid',\n","    label='Cumulative explained variance'\n",")\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal components')\n","plt.legend(loc='best')\n","plt.title('Scree Plot')\n","plt.show()\n","```\n","\n","#### **How to Interpret:**\n","- Look for the **\"elbow\"** in the scree plot (where the explained variance drops significantly).\n","- Choose the number of components that capture **~95% of the total variance**.\n","\n","---\n","\n","## **5. PCA for Dimensionality Reduction**\n","### **Example: Reduce to 2 Components**\n","```python\n","pca = PCA(n_components=2)\n","reduced_data = pca.fit_transform(scaled_data)\n","print(\"Reduced data shape:\", reduced_data.shape)  # (n_samples, 2)\n","```\n","\n","---\n","\n","## **6. PCA in Machine Learning**\n","PCA is often used as a **preprocessing step** to reduce the number of features before training a model.\n","\n","```python\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# Example: Load a dataset (e.g., Iris)\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Standardize the data\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","# Apply PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n","\n","# Train a model on the reduced data\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","\n","# Evaluate the model\n","print(\"Accuracy:\", model.score(X_test, y_test))\n","```\n","\n","---\n","\n","## **7. Common Pitfalls and Best Practices**\n","### **Pitfalls**\n","- **Not standardizing data**: PCA is sensitive to the scale of features. Always standardize your data first.\n","- **Over-relying on explained variance**: Don’t blindly keep components based on variance. Consider the interpretability and downstream task performance.\n","- **Ignoring the scree plot**: Always visualize the explained variance to make an informed decision.\n","\n","### **Best Practices**\n","- **Standardize your data** before applying PCA.\n","- **Use the scree plot** to decide the number of components.\n","- **Interpret the principal components** by examining the loadings (eigenvectors).\n","- **Test the impact** of PCA on your model’s performance.\n","\n","---\n","\n","## **8. Interpreting Principal Components**\n","The principal components are linear combinations of the original features. You can examine the **loadings** to understand what each component represents.\n","\n","```python\n","# Get the loadings (eigenvectors)\n","loadings = pca.components_\n","\n","# Create a DataFrame for the loadings\n","loadings_df = pd.DataFrame(\n","    loadings,\n","    columns=data.columns,\n","    index=[f'PC{i+1}' for i in range(loadings.shape[0])]\n",")\n","\n","print(\"Loadings:\\n\", loadings_df)\n","```\n","\n","#### **Example Output:**\n","```\n","Loadings:\n","          Feature1   Feature2   Feature3\n","PC1      0.707107  -0.707107   0.000000\n","PC2      0.408248   0.408248  -0.816497\n","```\n","- **PC1** is a contrast between `Feature1` and `Feature2`.\n","- **PC2** is influenced by all three features but most strongly by `Feature3`.\n","\n","---\n","\n","## **9. When to Use PCA**\n","- **Dimensionality Reduction**: Reduce the number of features while retaining most of the information.\n","- **Noise Reduction**: Remove less important components to focus on the signal.\n","- **Visualization**: Project high-dimensional data into 2D or 3D for plotting.\n","- **Feature Extraction**: Create new features that capture the most important patterns.\n","\n","---\n","\n","## **10. Alternatives to PCA**\n","- **t-SNE**: Better for visualization of high-dimensional data.\n","- **UMAP**: Preserves both local and global structure.\n","- **Factor Analysis**: Similar to PCA but assumes a statistical model for the data.\n","\n","---\n","\n","## **Summary**\n","- PCA is a powerful tool for **dimensionality reduction** and **feature extraction**.\n","- Always **standardize your data** before applying PCA.\n","- Use the **scree plot** to decide how many components to keep.\n","- PCA is widely used in **exploratory data analysis, machine learning, and visualization**."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.399402,"end_time":"2025-08-21T21:19:55.663698","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-21T21:19:49.264296","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}